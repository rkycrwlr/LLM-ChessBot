{
    "learning_rate": 0.0003,
    "weight_decay": 0.1,
    "gradient_accumulation_steps": 4,
    "batch_size": 64,
    "log_step": 20,
    "eval_steps": 500,
    "save_steps": 5000,
    "n_layer": 8,
    "n_head": 8,
    "n_embd": 512,
    "block_size": 768,
    "embd_pdrop": 0.1,
    "resid_pdrop": 0.1,
    "attn_pdrop": 0.1,
    "output_dir": "out"
}
